%pip install seaborn

# ======================================================
# üåæ Crop Recommendation using Soil & Weather Data
# Compare Random Forest & Gradient Boosting + Cross Validation (60/40 Split)
# ======================================================

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, classification_report, confusion_matrix
)
import matplotlib.pyplot as plt
import seaborn as sns

# 1Ô∏è‚É£ Load Dataset
file_path = 'Crop Recommendation using Soil Properties and Weather Prediction.csv'
df = pd.read_csv(file_path)
print("‚úÖ Dataset Loaded Successfully!")

# 2Ô∏è‚É£ Encode Target and Categorical Features
target_col = "label"
X = df.drop(columns=[target_col])
y = df[target_col]

le_target = LabelEncoder()
y_encoded = le_target.fit_transform(y)

le = LabelEncoder()
for col in X.columns:
    if X[col].dtype == 'object':
        X[col] = le.fit_transform(X[col])
        print(f"Encoded '{col}'")

# 3Ô∏è‚É£ Split Data (60% Train, 40% Test)
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.4, random_state=42, stratify=y_encoded
)
print(f"\n‚úÖ Train-Test Split Completed (60% train, 40% test)")
print(f"Training samples: {X_train.shape[0]}, Testing samples: {X_test.shape[0]}")

# 4Ô∏è‚É£ Scale Data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 5Ô∏è‚É£ Define Models
rf = RandomForestClassifier(n_estimators=200, random_state=42)
gb = GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42)

# 6Ô∏è‚É£ Cross Validation Setup
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

def evaluate_with_cv(model, name):
    print(f"\nüîÅ Performing 5-Fold Cross Validation for {name}...")
    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=cv, scoring='accuracy')
    print(f"Cross-validation accuracies: {cv_scores}")
    print(f"Mean CV Accuracy: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}")

    # Train + Predict
    model.fit(X_train_scaled, y_train)
    preds = model.predict(X_test_scaled)

    # Metrics
    acc = accuracy_score(y_test, preds)
    prec = precision_score(y_test, preds, average='weighted', zero_division=0)
    rec = recall_score(y_test, preds, average='weighted', zero_division=0)
    f1 = f1_score(y_test, preds, average='weighted', zero_division=0)

    print(f"\nüìä Test Performance of {name}:")
    print(f"Accuracy :  {acc:.4f}")
    print(f"Precision:  {prec:.4f}")
    print(f"Recall   :  {rec:.4f}")
    print(f"F1-Score :  {f1:.4f}")
    print("\nDetailed Classification Report:")
    print(classification_report(y_test, preds, target_names=le_target.classes_))

    # Confusion Matrix
    cm = confusion_matrix(y_test, preds)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, cmap='Blues', annot=False)
    plt.title(f"Confusion Matrix - {name}")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

    # Feature Importances
    importances = model.feature_importances_
    indices = np.argsort(importances)[::-1]
    plt.figure(figsize=(12, 6))
    sns.barplot(x=[X.columns[i] for i in indices], y=importances[indices], palette='viridis')
    plt.xticks(rotation=90)
    plt.title(f"Feature Importance - {name}")
    plt.tight_layout()
    plt.show()

    return {
        "Model": name,
        "CV_Mean": cv_scores.mean(),
        "CV_Std": cv_scores.std(),
        "Test_Acc": acc,
        "Precision": prec,
        "Recall": rec,
        "F1": f1
    }

# 7Ô∏è‚É£ Evaluate Both Models
rf_results = evaluate_with_cv(rf, "Random Forest")
gb_results = evaluate_with_cv(gb, "Gradient Boosting")

# 8Ô∏è‚É£ Summary Table
results_df = pd.DataFrame([rf_results, gb_results])
print("\n============================")
print("üìà Final Comparison Summary (60/40 Split)")
print("============================")
print(results_df)

print("\nüèÅ Both models trained & evaluated successfully. Choose based on best test or CV accuracy.")
